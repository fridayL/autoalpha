{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** {'amap_weather': <class 'autoalpha.tools.amap_weather.AmapWeather'>}\n"
     ]
    }
   ],
   "source": [
    "from autoalpha.agent.assistant_agent import  AssistantAgent\n",
    "from autoalpha.agent.groupchat import  GroupChat\n",
    "configs = {\n",
    "    \"agents\": [\n",
    "        {\"name\": \"爬虫工程师\",\n",
    "        \"selected_tools\": [\"amap_weather\"],\n",
    "        \"description\": \"擅长调用工具爬取天气信息\",\n",
    "        \"llm\":  {\n",
    "        \"model\":\"moonshot-v1-8k\",\n",
    "        'model_server': 'https://api.moonshot.cn/v1',  # api_base\n",
    "        'api_key': 'sk-rm4Mek55xss0Endd1iYRzi6yu9OZRCU0ES597mMtXYP9N1Z5',\n",
    "        }},\n",
    "    {\"name\": \"问题拆解工程师\",\n",
    "        \"selected_tools\": [],\n",
    "        \"description\": \"擅长将用户的问题拆解并指挥爬虫工程师，同时能够将结果结构化\",\n",
    "        \"llm\":  {\n",
    "        \"model\":\"moonshot-v1-8k\",\n",
    "        'model_server': 'https://api.moonshot.cn/v1',  # api_base\n",
    "        'api_key': 'sk-rm4Mek55xss0Endd1iYRzi6yu9OZRCU0ES597mMtXYP9N1Z5',\n",
    "        }}\n",
    "]}\n",
    "\n",
    "assistant = GroupChat(\n",
    "    configs=configs,\n",
    "    llm={\n",
    "    \"model\":\"moonshot-v1-8k\",\n",
    "    'model_server': 'https://api.moonshot.cn/v1',  # api_base\n",
    "    'api_key': 'sk-rm4Mek55xss0Endd1iYRzi6yu9OZRCU0ES597mMtXYP9N1Z5',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  0\n",
      "message, name, output 问题拆解工程师 [Message({'role': 'system', 'content': '这是一个项目组,项目是为了解决用户问题 \\n你是问题拆解工程师。你的简介是：擅长将用户的问题拆解并指挥爬虫工程师，同时能够将结果结构化\\n\\n\\n\\n项目他成员包括：爬虫工程师\\n\\n                    你的输出有三种格式选择：\\n                    1. 完成其他成员交给你的task并以```json{{\"content\": \"\", \"task_from\": \"\"}}```返回结果\\n                    2. 要求其他成员完成某项task并以```json{{\"content\": \"\", \"task_to\": \"\"}}\\n                    3. 如果你要使用工具请按照工具要求进行参数解析\\n                    注: task_from 和task_to必须是 [{other_agents}] 里面的\\n                    '}), Message({'role': 'user', 'content': '需要整理一下北京市的天气，并输出成json格式'})] [Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"请爬虫工程师抓取北京市的实时天气数据。\",\\n  \"task_to\": \"爬虫工程师\"\\n}\\n```'})]\n",
      "round  1\n",
      "[{'content': '请爬虫工程师抓取北京市的实时天气数据。', 'task_to': '爬虫工程师'}]\n",
      "message, name, output 爬虫工程师 [Message({'role': 'system', 'content': '这是一个项目组,项目是为了解决用户问题 \\n你是爬虫工程师。你的简介是：擅长调用工具爬取天气信息\\n\\n\\n\\n项目他成员包括：问题拆解工程师\\n\\n                    你的输出有三种格式选择：\\n                    1. 完成其他成员交给你的task并以```json{{\"content\": \"\", \"task_from\": \"\"}}```返回结果\\n                    2. 要求其他成员完成某项task并以```json{{\"content\": \"\", \"task_to\": \"\"}}\\n                    3. 如果你要使用工具请按照工具要求进行参数解析\\n                    注: task_from 和task_to必须是 [{other_agents}] 里面的\\n                    '}), Message({'role': 'user', 'content': '需要整理一下北京市的天气，并输出成json格式'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"请爬虫工程师抓取北京市的实时天气数据。\",\\n  \"task_to\": \"爬虫工程师\"\\n}\\n```'})] [Message({'role': 'assistant', 'content': [], 'function_call': {'name': 'amap_weather', 'arguments': [{'name': 'location', 'value': '北京市'}]}})]\n",
      "round  2\n",
      "[]\n",
      "message, name, output None [Message({'role': 'system', 'content': '\\n在一个角色扮演中你是一个负责控制发言的角色，你可以根据角色对话控制下一轮对话的角色以及要做的事情,并且每次只能最多选一个speaker\\n在任务中有如下角色：\\n爬虫工程师:擅长调用工具爬取天气信息\\n\\n问题拆解工程师:擅长将用户的问题拆解并指挥爬虫工程师，同时能够将结果结构化\\n\\n\\n最终输出结果为：\\njson```{\"speaker\":\"\", \"task\":\"\"}```\\n其中\\nspeaker: 必须是[爬虫工程师,问题拆解工程师] 里面的\\ntask: 是需要执行具体的任务\\n如果你觉得根据历史对话已经得到了答案,请只输出以下内容\\njson```{\"speaker\":\"FINISHED\", \"task\":\"最终输出结果\"}```\\n'}), Message({'role': 'user', 'content': '需要整理一下北京市的天气，并输出成json格式'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"请爬虫工程师抓取北京市的实时天气数据。\",\\n  \"task_to\": \"爬虫工程师\"\\n}\\n```'}), Message({'role': 'assistant', 'content': '北京市的天气是多云温度是20度。'})] [Message({'role': 'assistant', 'content': '```json\\n{\\n  \"speaker\": \"爬虫工程师\",\\n  \"task\": \"抓取北京市的实时天气数据\"\\n}\\n```\\n\\n```json\\n{\\n  \"speaker\": \"问题拆解工程师\",\\n  \"task\": \"将爬虫工程师抓取到的北京市天气数据整理成JSON格式\"\\n}\\n```\\n\\n```json\\n{\\n  \"weather\": \"多云\",\\n  \"temperature\": \"20度\",\\n  \"city\": \"北京市\"\\n}\\n```'})]\n",
      "[{'speaker': '爬虫工程师', 'task': '抓取北京市的实时天气数据'}, {'speaker': '问题拆解工程师', 'task': '将爬虫工程师抓取到的北京市天气数据整理成JSON格式'}, {'weather': '多云', 'temperature': '20度', 'city': '北京市'}]\n",
      "message, name, output 问题拆解工程师 [Message({'role': 'system', 'content': '这是一个项目组,项目是为了解决用户问题 \\n你是问题拆解工程师。你的简介是：擅长将用户的问题拆解并指挥爬虫工程师，同时能够将结果结构化\\n\\n\\n\\n项目他成员包括：爬虫工程师\\n\\n                    你的输出有三种格式选择：\\n                    1. 完成其他成员交给你的task并以```json{{\"content\": \"\", \"task_from\": \"\"}}```返回结果\\n                    2. 要求其他成员完成某项task并以```json{{\"content\": \"\", \"task_to\": \"\"}}\\n                    3. 如果你要使用工具请按照工具要求进行参数解析\\n                    注: task_from 和task_to必须是 [{other_agents}] 里面的\\n                    '}), Message({'role': 'user', 'content': '需要整理一下北京市的天气，并输出成json格式'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"请爬虫工程师抓取北京市的实时天气数据。\",\\n  \"task_to\": \"爬虫工程师\"\\n}\\n```'}), Message({'role': 'assistant', 'content': '北京市的天气是多云温度是20度。'})] [Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"北京市的天气多云，温度20度。\",\\n  \"task_from\": \"用户\"\\n}\\n```'})]\n",
      "round  3\n",
      "[{'content': '北京市的天气多云，温度20度。', 'task_from': '用户'}]\n",
      "message, name, output None [Message({'role': 'system', 'content': '\\n在一个角色扮演中你是一个负责控制发言的角色，你可以根据角色对话控制下一轮对话的角色以及要做的事情,并且每次只能最多选一个speaker\\n在任务中有如下角色：\\n爬虫工程师:擅长调用工具爬取天气信息\\n\\n问题拆解工程师:擅长将用户的问题拆解并指挥爬虫工程师，同时能够将结果结构化\\n\\n\\n最终输出结果为：\\njson```{\"speaker\":\"\", \"task\":\"\"}```\\n其中\\nspeaker: 必须是[爬虫工程师,问题拆解工程师] 里面的\\ntask: 是需要执行具体的任务\\n如果你觉得根据历史对话已经得到了答案,请只输出以下内容\\njson```{\"speaker\":\"FINISHED\", \"task\":\"最终输出结果\"}```\\n'}), Message({'role': 'user', 'content': '需要整理一下北京市的天气，并输出成json格式'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"请爬虫工程师抓取北京市的实时天气数据。\",\\n  \"task_to\": \"爬虫工程师\"\\n}\\n```'}), Message({'role': 'assistant', 'content': '北京市的天气是多云温度是20度。'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"北京市的天气多云，温度20度。\",\\n  \"task_from\": \"用户\"\\n}\\n```'})] [Message({'role': 'assistant', 'content': '```json\\n{\\n  \"speaker\": \"问题拆解工程师\",\\n  \"task\": \"将北京市的天气信息整理成JSON格式\"\\n}\\n```'})]\n",
      "[{'speaker': '问题拆解工程师', 'task': '将北京市的天气信息整理成JSON格式'}]\n",
      "message, name, output 问题拆解工程师 [Message({'role': 'system', 'content': '这是一个项目组,项目是为了解决用户问题 \\n你是问题拆解工程师。你的简介是：擅长将用户的问题拆解并指挥爬虫工程师，同时能够将结果结构化\\n\\n\\n\\n项目他成员包括：爬虫工程师\\n\\n                    你的输出有三种格式选择：\\n                    1. 完成其他成员交给你的task并以```json{{\"content\": \"\", \"task_from\": \"\"}}```返回结果\\n                    2. 要求其他成员完成某项task并以```json{{\"content\": \"\", \"task_to\": \"\"}}\\n                    3. 如果你要使用工具请按照工具要求进行参数解析\\n                    注: task_from 和task_to必须是 [{other_agents}] 里面的\\n                    '}), Message({'role': 'user', 'content': '需要整理一下北京市的天气，并输出成json格式'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"请爬虫工程师抓取北京市的实时天气数据。\",\\n  \"task_to\": \"爬虫工程师\"\\n}\\n```'}), Message({'role': 'assistant', 'content': '北京市的天气是多云温度是20度。'}), Message({'role': 'assistant', 'content': '```json\\n{\\n  \"content\": \"北京市的天气多云，温度20度。\",\\n  \"task_from\": \"用户\"\\n}\\n```'})] [Message({'role': 'assistant', 'content': '好的，我将整理结果并输出为JSON格式。\\n\\n```json\\n{\\n  \"city\": \"北京市\",\\n  \"weather\": \"多云\",\\n  \"temperature\": \"20度\"\\n}\\n```'})]\n",
      "round  4\n",
      "[{'city': '北京市', 'weather': '多云', 'temperature': '20度'}]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'max request per minute reached: 5, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m需要整理一下北京市的天气，并输出成json格式\u001b[39m\u001b[39m\"\u001b[39m}]\n\u001b[0;32m      2\u001b[0m data \u001b[39m=\u001b[39m assistant\u001b[39m.\u001b[39mrun(messages)\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m dt \u001b[39min\u001b[39;00m data:\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(dt)\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\agent\\groupchat.py:75\u001b[0m, in \u001b[0;36mGroupChat.run\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m         trans_object\u001b[39m.\u001b[39mappend(msg)\n\u001b[1;32m---> 75\u001b[0m \u001b[39mfor\u001b[39;00m rsp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(messages\u001b[39m=\u001b[39;49mtrans_object):\n\u001b[0;32m     76\u001b[0m     \u001b[39myield\u001b[39;00m rsp\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\agent\\groupchat.py:98\u001b[0m, in \u001b[0;36mGroupChat._run\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m     96\u001b[0m new_msg \u001b[39m=\u001b[39m []\n\u001b[0;32m     97\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_llm(messages)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m     99\u001b[0m     new_msg\u001b[39m.\u001b[39mappend(output)\n\u001b[0;32m    100\u001b[0m agents_turn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_agents(messages\u001b[39m=\u001b[39mnew_msg)\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\agent\\manager_agent.py:85\u001b[0m, in \u001b[0;36mAgentManager._call_llm\u001b[1;34m(self, messages, functions, stream)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(messages[\u001b[39m0\u001b[39m][CONTENT], \u001b[39mstr\u001b[39m):\n\u001b[0;32m     84\u001b[0m     messages[\u001b[39m0\u001b[39m][CONTENT] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msystem_message \u001b[39m+\u001b[39m messages[\u001b[39m0\u001b[39m][CONTENT]   \n\u001b[1;32m---> 85\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mchat(messages\u001b[39m=\u001b[39;49mmessages,\n\u001b[0;32m     86\u001b[0m                      functions\u001b[39m=\u001b[39;49mfunctions,\n\u001b[0;32m     87\u001b[0m                      stream\u001b[39m=\u001b[39;49mstream)\n\u001b[0;32m     88\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmessage, name, output\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, messages, outputs) \n\u001b[0;32m     89\u001b[0m output \u001b[39m=\u001b[39m outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\oai\\base.py:49\u001b[0m, in \u001b[0;36mBaseChat.chat\u001b[1;34m(self, messages, functions, stream)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs\n\u001b[0;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chat(messages, functions, fncall_mode)\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\oai\\base.py:54\u001b[0m, in \u001b[0;36mBaseChat._chat\u001b[1;34m(self, messages, functions, fncall_mode)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chat\u001b[39m(\u001b[39mself\u001b[39m, messages: List[Message],\n\u001b[0;32m     52\u001b[0m           functions: Optional[List[Dict]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m     53\u001b[0m           fncall_mode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chat_no_stream(messages)\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\oai\\openai_chat.py:104\u001b[0m, in \u001b[0;36mOpenaiChat._chat_no_stream\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chat_no_stream\u001b[39m(\u001b[39mself\u001b[39m, messages: List[Message]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Message]:\n\u001b[0;32m    103\u001b[0m     messages \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mmodel_dump() \u001b[39mfor\u001b[39;00m msg \u001b[39min\u001b[39;00m messages]\n\u001b[1;32m--> 104\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chat_complete_create(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m    105\u001b[0m                                             messages\u001b[39m=\u001b[39mmessages,\n\u001b[0;32m    106\u001b[0m                                             stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    107\u001b[0m                                             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_cfg)\n\u001b[0;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m [Message(ASSISTANT, response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)]\n",
      "File \u001b[1;32md:\\workspace\\autoalpha\\autoalpha\\oai\\openai_chat.py:98\u001b[0m, in \u001b[0;36mOpenaiChat.__init__.<locals>._chat_complete_create\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chat_complete_create\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     97\u001b[0m     client \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mOpenAI(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mapi_kwargs)\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_utils\\_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\resources\\chat\\completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    610\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    657\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    658\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 659\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    660\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    661\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    662\u001b[0m             {\n\u001b[0;32m    663\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m    664\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    665\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m    666\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[0;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[0;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m    669\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[0;32m    670\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m    671\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    672\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m    673\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    674\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m    675\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m    676\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m    677\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m    678\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m    679\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    680\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[0;32m    681\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m    682\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    683\u001b[0m             },\n\u001b[0;32m    684\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[0;32m    685\u001b[0m         ),\n\u001b[0;32m    686\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    687\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    688\u001b[0m         ),\n\u001b[0;32m    689\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m    690\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    691\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m    692\u001b[0m     )\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1167\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1168\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1176\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1177\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1178\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1179\u001b[0m     )\n\u001b[1;32m-> 1180\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    861\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    868\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 869\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    870\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    871\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    872\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    873\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    874\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    875\u001b[0m     )\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:945\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m    944\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 945\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    946\u001b[0m         options,\n\u001b[0;32m    947\u001b[0m         cast_to,\n\u001b[0;32m    948\u001b[0m         retries,\n\u001b[0;32m    949\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    950\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    951\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    952\u001b[0m     )\n\u001b[0;32m    954\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 993\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    994\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    995\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    996\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m    997\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    998\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    999\u001b[0m )\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:945\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[0;32m    944\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[1;32m--> 945\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[0;32m    946\u001b[0m         options,\n\u001b[0;32m    947\u001b[0m         cast_to,\n\u001b[0;32m    948\u001b[0m         retries,\n\u001b[0;32m    949\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    950\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    951\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    952\u001b[0m     )\n\u001b[0;32m    954\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:993\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 993\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    994\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    995\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    996\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[0;32m    997\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    998\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    999\u001b[0m )\n",
      "File \u001b[1;32md:\\program\\anaconda\\envs\\nlp\\lib\\site-packages\\openai\\_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    957\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m    959\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[0;32m    963\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[0;32m    964\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    967\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[0;32m    968\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'max request per minute reached: 5, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"需要整理一下北京市的天气，并输出成json格式\"}]\n",
    "data = assistant.run(messages)\n",
    "for dt in data:\n",
    "    print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
